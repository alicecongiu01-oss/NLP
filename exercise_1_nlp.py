# -*- coding: utf-8 -*-
"""Exercise 1 - NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BGrtZLpquVjSY7umstHnHNZaPO0PzUQm
"""

#load texts from https://www.gutenberg.org/
import urllib.request

text_en_url = "https://www.gutenberg.org/cache/epub/31270/pg31270.txt" # The Writings of Thomas Paine
text_it_url = "https://www.gutenberg.org/cache/epub/62302/pg62302.txt" # Scritti Politici

text_en = urllib.request.urlopen(text_en_url).read()
text_it = urllib.request.urlopen(text_it_url).read()

# decode texts with UTF-8 Encoding
text_en = text_en.decode("utf-8")
text_it = text_it.decode("utf-8")

# preprocessing
import re

# start and end phrases
# en
start_phrase_en = "THE WRITINGS OF THOMAS PAINE, VOLUME I."
end_phrase_en   = "END OF PART II"

#it
start_phrase_it = "SCRITTI POLITICI"
end_phrase_it = "FINE."

# extract main content
# en
start_en = text_en.find(start_phrase_en)
end_en = text_en.find(end_phrase_en)
text_en_clean = text_en[start_en:end_en + len(end_phrase_en)] # include end_en

# it
start_it = text_it.find(start_phrase_it)
end_it = text_it.find(end_phrase_it)
text_it_clean = text_it[start_it:end_it + len(end_phrase_it)] # include end_it

# remove whitespaces
text_en_clean = re.sub(r"\s+", " ", text_en_clean).strip()
text_it_clean = re.sub(r"\s+", " ", text_it_clean).strip()

#import spacy
import spacy

# install pipelines and create Doc object
# en_core_news_sm
!python -m spacy download en_core_web_sm
import en_core_web_sm
nlp_en = en_core_web_sm.load()
doc_en = nlp_en(text_en_clean)

# it_core_news_sm
!python -m spacy download it_core_news_sm
import it_core_news_sm
nlp_it = it_core_news_sm.load()
nlp_it.max_length = 1114143 # increase max_lenght to accomodate text
doc_it = nlp_it(text_it_clean)

#tokenisation
# en
words_en = [token.lower_ # all tokens in lower case
            for token in doc_en
            if token.is_alpha] # all tokens are alphabetic characters

# it
words_it = [token.lower_ # all tokens in lower case
            for token in doc_it
            if token.is_alpha] # all tokens are alphabetic characters

# Compute tokens and types
# en
num_tokens_en = len(words_en)
print("Total English tokens: ", num_tokens_en)
types_en = len(set(words_en))
print("English types:", types_en)

print()

# it
num_tokens_it = len(words_it)
print("Total Italian tokens:", num_tokens_it)
types_it = len(set(words_it))
print("Italian types: ", types_it)

# set up the envoironment for Zipf's law of abbreviation
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="darkgrid")

# df with words and frequencies
# en
df_en = pd.DataFrame.from_records(list(dict(Counter(words_en)).items()), columns = ["word", "frequency"])
df_en

# df with words and frequencies
# it
df_it = pd.DataFrame.from_records(list(dict(Counter(words_it)).items()), columns = ["word", "frequency"])
df_it

# order words by frequency (descending order) and assign rank
# en
df_en = df_en.sort_values(by=["frequency"], ascending=False)
df_en["rank"] = list(range(1, len(df_en) + 1))
df_en

# order words by frequency (descending order) and assign rank
# it
df_it = df_it.sort_values(by=["frequency"], ascending=False)
df_it["rank"] = list(range(1, len(df_it) + 1))
df_it

# en linear graph
sns.relplot(x="rank", y="frequency", data=df_en);
plt.show()
plt.close()

# it linear graph
sns.relplot(x="rank", y="frequency", data=df_it);
plt.show()
plt.close()

# import log to compress frequencies
import math
from math import log

# en log frequencies
df_en["logfreq"] = [log(x+1) for x in df_en["frequency"]]
df_en

# en log frequecies graph
sns.relplot(x="rank", y="logfreq", data=df_en);
plt.show()
plt.close()

# it log frequencies
df_it["logfreq"] = [log(x+1) for x in df_it["frequency"]]
df_it

# it log frequencies graph
sns.relplot(x="rank", y="logfreq", data=df_it);
plt.show()
plt.close()